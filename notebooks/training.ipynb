{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2379306",
   "metadata": {},
   "source": [
    "### Tutorial: Training Demo\n",
    "\n",
    "This notebook demonstrates the HME training pipeline using a minimal setup. \n",
    "\n",
    "**⚠️ Important:**\n",
    "This notebook is for **educational purposes** to understand the code structure and data flow. For actual large-scale training (multi-GPU, DeepSpeed), please use the provided shell scripts in `scripts/`.\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare a tiny dummy dataset.\n",
    "2. Configure training arguments (CPU/Single-GPU mode).\n",
    "3. Run the training loop for a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9bbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the training function\n",
    "from hme.run_clm import train\n",
    "\n",
    "# Output directory for this demo\n",
    "DEMO_OUTPUT_DIR = Path(\"demo_training_output\")\n",
    "DEMO_OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbfac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found training data: ../datasets/property_qa_test_2.json\n",
      "Found embeddings: ./demo_subset_1000.json.cfm.pt\n"
     ]
    }
   ],
   "source": [
    "# We will use the data generated in Tutorial 01\n",
    "json_data_path = \"../datasets/property_qa_test_2.json\"\n",
    "pt_data_path = \"./demo_subset_1000.json.cfm.pt\"\n",
    "\n",
    "# Validate that prerequisites are met\n",
    "if not os.path.exists(json_data_path) or not os.path.exists(pt_data_path):\n",
    "    print(\"Error: Demo data not found.\")\n",
    "    print(\"Please run 'data_preprocess.ipynb' first to generate the demo subset.\")\n",
    "    # Stop execution of subsequent cells if data is missing\n",
    "    raise FileNotFoundError\n",
    "else:\n",
    "    print(f\"Found training data: {json_data_path}\")\n",
    "    print(f\"Found embeddings: {pt_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48379971",
   "metadata": {},
   "source": [
    "### 1. Configure Training Arguments\n",
    "\n",
    "We translate the parameters from `scripts/run_zero2_comprehension-pretrain.sh` into Python arguments.\n",
    "\n",
    "**Key Adjustments for Demo:**\n",
    "*   `--num_train_epochs 1`\n",
    "*   `--max_steps 3` (Stop after 3 steps)\n",
    "*   Disable DeepSpeed (Run standard PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a9b70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training demo with real data...\n",
      "(This may take a minute to load the model...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vocab_file: vocab_800_other_tasks.txt to load fragment list.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3d74e334674c0d915c511a010f8774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 267,644,928 || all params: 8,568,729,600 || trainable%: 3.1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/llzh/NC_minor/HME/src/hme/data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(emb_dict_mol, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now the length of the dataset is 4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-08 07:59:17,527] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/miniconda3/envs/mollama/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/lvliuzhenghao/miniconda3/envs/mollama/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.935000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.962300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.828300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.191100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.455500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/miniconda3/envs/mollama/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demo training finished successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simulate command line arguments\n",
    "# Note: We point 'base_model_path' to the downloaded Llama-3 folder\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use GPU 0 if available\n",
    "base_model_path = \"../checkpoints/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "if not os.path.exists(base_model_path):\n",
    "    print(f\"⚠️ Warning: Base model not found at {base_model_path}. Please run Tutorial 02 first.\")\n",
    "else:\n",
    "    # Construct arguments list\n",
    "    # These map directly to HfArgumentParser fields in hme.run_clm.py\n",
    "    sys.argv = [\n",
    "        \"hme.run_clm\",\n",
    "        \"--model_name_or_path\", base_model_path,\n",
    "        \"--output_dir\", str(DEMO_OUTPUT_DIR),\n",
    "        \n",
    "        # Data Config (Using Real Demo Data)\n",
    "        \"--data_path\", json_data_path,   # <--- Updated\n",
    "        \"--task_type\", \"qa\",\n",
    "        \"--data_type\", \"1d,2d,3d,frg\",\n",
    "        \"--emb_dict_mol\", pt_data_path,  # <--- Updated\n",
    "        \"--emb_dict_protein\", \"none\",\n",
    "        \n",
    "        # LoRA Config\n",
    "        \"--lora_r\", \"8\",\n",
    "        \"--lora_alpha\", \"16\",\n",
    "        \"--lora_targets\", \"q_proj,v_proj\",\n",
    "        \"--modules_to_save\", \"feature_fuser\",\n",
    "        \"--merge_when_finished\", \"False\",\n",
    "        \n",
    "        # Training Config (Minimal for Speed)\n",
    "        \"--max_length\", \"128\",         # Shorter length for speed\n",
    "        \"--per_device_train_batch_size\", \"1\", # Batch size 1 for CPU/Low-mem GPU compatibility\n",
    "        \"--gradient_accumulation_steps\", \"1\",\n",
    "        \"--learning_rate\", \"1e-4\",\n",
    "        \"--num_train_epochs\", \"1\",\n",
    "        \"--max_steps\", \"20\",            # Only run 20 steps to verify the loop\n",
    "        \"--save_strategy\", \"no\",\n",
    "        \"--logging_steps\", \"1\",        # Log every step to show progress immediately\n",
    "        \"--report_to\", \"none\",\n",
    "        \"--bf16\", \"False\",\n",
    "        \"--do_train\"\n",
    "    ]\n",
    "\n",
    "    print(\"Starting training demo with real data...\")\n",
    "    print(\"(This may take a minute to load the model...)\")\n",
    "    \n",
    "    train()\n",
    "    print(\"\\nDemo training finished successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
