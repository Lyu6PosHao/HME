{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e257a3",
   "metadata": {},
   "source": [
    "### Tutorial: Download Model Checkpoints\n",
    "\n",
    "This notebook guides you through downloading the pre-trained weights required to run HME.\n",
    "\n",
    "**HME consists of two parts:**\n",
    "1. **Base LLM:** `Meta-Llama-3-8B-Instruct` (Requires Hugging Face authentication).\n",
    "2. **HME Adapters:** Specialized checkpoints for different tasks (Captioning, QA, etc.).\n",
    "\n",
    "You can choose to download all models or specific ones based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c815aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models will be downloaded to: ../checkpoints/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the core download function\n",
    "from hme.download_models import download_model\n",
    "\n",
    "# Define the root directory for saving checkpoints\n",
    "CHECKPOINTS_DIR = Path('../checkpoints')\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Models will be downloaded to: ../checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408096d9",
   "metadata": {},
   "source": [
    "### 1. Download Base Model (Llama-3)\n",
    "\n",
    "**Important Requirement:**\n",
    "To download `meta-llama/Meta-Llama-3-8B-Instruct`, you must:\n",
    "1. Have a Hugging Face account.\n",
    "2. Request access on the [model page](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    "3. Log in locally by running `huggingface-cli login` in your terminal or using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec21d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Base Model: meta-llama/Meta-Llama-3-8B-Instruct ...\n",
      "Downloading 'meta-llama/Meta-Llama-3-8B-Instruct' to '../checkpoints/Meta-Llama-3-8B-Instruct'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/miniconda3/envs/mollama/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/lvliuzhenghao/miniconda3/envs/mollama/lib/python3.10/site-packages/huggingface_hub/file_download.py:980: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab7e39bff9f4031bfd146080447afaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55403f3d92e489982141082957c7b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/7.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to download 'meta-llama/Meta-Llama-3-8B-Instruct'. Error: 403 Client Error. (Request ID: Root=1-693674d6-4d150027479460c4127f4d71;08e44720-b0c2-4ce1-83db-42ff420dcc38)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/8afb486c1db24fe5011ec46dfbe5b5dccdb575c2/config.json.\n",
      "Your request to access model meta-llama/Meta-Llama-3-8B-Instruct has been rejected by the repo's authors.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/miniconda3/envs/mollama/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line if you haven't logged in yet\n",
    "# !huggingface-cli login\n",
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "local_name = \"Meta-Llama-3-8B-Instruct\"\n",
    "target_dir = CHECKPOINTS_DIR / local_name\n",
    "\n",
    "print(f\"Downloading Base Model: {repo_id} ...\")\n",
    "try:\n",
    "    download_model(repo_id, target_dir)\n",
    "    print(\"Base model ready.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nDownload Failed: {e}\")\n",
    "    print(\"Please check your Hugging Face login status and access permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a69d8",
   "metadata": {},
   "source": [
    "### 2. Download HME Task Adapters\n",
    "\n",
    "Select the models you wish to use. You can uncomment specific lines in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8669fd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download for 2 selected adapters...\n",
      "\n",
      "Downloading [HME_comprehension-pretrain] from GreatCaptainNemo/HME_comprehension-pretrain...\n",
      "Downloading 'GreatCaptainNemo/HME_comprehension-pretrain' to '../checkpoints/HME_comprehension-pretrain'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854a1c256cea4b72ab48994b9b57dfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab20059d02e4815b13620a088295fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87c51bb30af487e878158afea2f9523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8914b4f8cd5148a38aabfc5cfa6d2ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec902f748e70442ab2dd11a234fd1e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49583bafb384b1ea004191cb9246156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 'GreatCaptainNemo/HME_comprehension-pretrain'.\n",
      "Downloading [HME_general-qa] from GreatCaptainNemo/HME_general-qa...\n",
      "Downloading 'GreatCaptainNemo/HME_general-qa' to '../checkpoints/HME_general-qa'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a6db08bcdb45fa9e0efefd5f89787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7fc8d3815a4374be4a7b004b6afc45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/612M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecc098ee97f4b608b643f598f40c518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca817b253a9a471ca49b79cf65dd6fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c48b0acc884199a89ac0ead2f25827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588e2c1f5c7247cf891b973fa2ab24dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded 'GreatCaptainNemo/HME_general-qa'.\n",
      "\n",
      "All selected tasks processed.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of available HME models\n",
    "hme_models = {\n",
    "    \"HME_comprehension-pretrain\": \"GreatCaptainNemo/HME_comprehension-pretrain\",\n",
    "    # \"HME_comprehension-pretrain-s2\": \"GreatCaptainNemo/HME_comprehension-pretrain-s2\",\n",
    "    #\"HME_captioning\": \"GreatCaptainNemo/HME_captioning\",\n",
    "    \"HME_general-qa\": \"GreatCaptainNemo/HME_general-qa\",\n",
    "    # \"HME_property-qa-1\": \"GreatCaptainNemo/HME_property-qa-1\",\n",
    "    # \"HME_property-qa-2\": \"GreatCaptainNemo/HME_property-qa-2\",\n",
    "    # \"HME_pocket-based-ligand-generation_pretrain\": \"GreatCaptainNemo/HME_pocket-based-ligand-generation_pretrain\",\n",
    "    # \"HME_pocket-based-ligand-generation\": \"GreatCaptainNemo/HME_pocket-based-ligand-generation\",\n",
    "}\n",
    "\n",
    "print(f\"Starting download for {len(hme_models)} selected adapters...\\n\")\n",
    "\n",
    "for local_name, repo_id in hme_models.items():\n",
    "    target_dir = CHECKPOINTS_DIR / local_name\n",
    "    print(f\"Downloading [{local_name}] from {repo_id}...\")\n",
    "    \n",
    "    try:\n",
    "        download_model(repo_id, target_dir)\n",
    "    except SystemExit:\n",
    "        # Catch sys.exit from the helper function to prevent notebook from stopping\n",
    "        print(f\"Failed to download {local_name}\")\n",
    "        \n",
    "print(\"\\nAll selected tasks processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b4b55",
   "metadata": {},
   "source": [
    "### 3. Verification\n",
    "\n",
    "Check if the directories were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b17c1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of checkpoints directory:\n",
      "  ðŸ“‚ Meta-Llama-3-8B-Instruct\n",
      "  ðŸ“‚ HME_comprehension-pretrain\n",
      "  ðŸ“‚ HME_general-qa\n"
     ]
    }
   ],
   "source": [
    "print(\"Contents of checkpoints directory:\")\n",
    "for item in CHECKPOINTS_DIR.iterdir():\n",
    "    if item.is_dir():\n",
    "        print(f\"  ðŸ“‚ {item.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
