{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b4db81",
   "metadata": {},
   "source": [
    "### Tutorial: Merge LoRA Adapters\n",
    "\n",
    "HME uses a multi-stage training strategy, which often requires merging previous stage adapters into the base model to serve as the starting point for the next stage or for efficient inference.\n",
    "\n",
    "**Why Merge?**\n",
    "1. **Inference Efficiency:** Merged models have lower latency than loading base + adapter separately.\n",
    "2. **Dependency Chain:** Some downstream tasks (like QA) are fine-tuned on top of the comprehension-pretrained model. You must create the `_merged` version of the parent model first.\n",
    "\n",
    "**Workflow:**\n",
    "This notebook allows you to define the Base Model and Adapter, then performs the merge operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885d2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import core merging logic\n",
    "from hme.merge import merge_lora_model\n",
    "\n",
    "# Directories\n",
    "CHECKPOINTS_DIR = Path('../checkpoints')\n",
    "BASE_MODEL_NAME = \"Meta-Llama-3-8B-Instruct\"  # Change this if your base model folder differs\n",
    "BASE_MODEL_PATH = CHECKPOINTS_DIR / BASE_MODEL_NAME\n",
    "\n",
    "if not BASE_MODEL_PATH.exists():\n",
    "    print(f\"Warning: Base model not found at {BASE_MODEL_PATH}. Please run 'download_checkpoints.ipynb' first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db904fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the argument classes and the merge function\n",
    "from hme.merge import merge_lora_model, ModelArguments, DataArguments\n",
    "\n",
    "def run_merge(base_path, adapter_name, task_type=None):\n",
    "    \"\"\"\n",
    "    Helper to prepare arguments and run the merge.\n",
    "    \"\"\"\n",
    "    adapter_path = CHECKPOINTS_DIR / adapter_name\n",
    "    # Auto-generate output path\n",
    "    output_path = CHECKPOINTS_DIR / f\"{adapter_name}_merged\"\n",
    "    \n",
    "    # Check existence\n",
    "    if not adapter_path.exists():\n",
    "        print(f\"Skipping: Adapter {adapter_name} not found.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Merging: {base_path.name} + {adapter_name} -> {output_path.name}\")\n",
    "    \n",
    "    # Instantiate arguments directly\n",
    "    model_args = ModelArguments(\n",
    "        model_name_or_path=str(base_path),\n",
    "        peft_model_path=str(adapter_path),\n",
    "        merged_model_path=str(output_path)\n",
    "    )\n",
    "    data_args = DataArguments(task_type=task_type)\n",
    "    \n",
    "    # Execute\n",
    "    merge_lora_model(model_args, data_args)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23679d4",
   "metadata": {},
   "source": [
    "### 1. Execute Merge Chain (Demo: General QA)\n",
    "\n",
    "We will demonstrate the merge process using the **General QA** task. This is a multi-stage merge:\n",
    "1. **Stage 1:** Merge `Comprehension Pretrain` adapter into the Base Llama-3 model.\n",
    "2. **Stage 2:** Merge `General QA` adapter into the result of Stage 1.\n",
    "\n",
    "> **Note:** For other tasks (e.g., Pocket-based Generation, Property QA), please refer to `scripts/merge_models.sh` for the complete dependency chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4447f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vocab_file: vocab_800_other_tasks.txt to load fragment list.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stage 1: Merging Pretrain Adapter ---\n",
      "Merging: Meta-Llama-3-8B-Instruct + HME_comprehension-pretrain -> HME_comprehension-pretrain_merged\n",
      "--- Loading Base Model and Tokenizer ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bd78d34e524a328018002cebcf9653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading PEFT Adapter from ../checkpoints/HME_comprehension-pretrain ---\n",
      "--- Merging Adapter into Base Model ---\n",
      "--- Saving Merged Model to ../checkpoints/HME_comprehension-pretrain_merged ---\n",
      "[2025-12-08 07:20:51,027] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/miniconda3/envs/mollama/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/lvliuzhenghao/miniconda3/envs/mollama/bin/../lib/gcc/x86_64-conda-linux-gnu/11.2.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING:hme.util:Using vocab_file: vocab_800_other_tasks.txt to load fragment list.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No task-specific head to save (task is likely conditional generation).\n",
      "--- Model merged and saved successfully to ../checkpoints/HME_comprehension-pretrain_merged ---\n",
      "\n",
      "--- Stage 2: Merging General QA Adapter ---\n",
      "Merging: HME_comprehension-pretrain_merged + HME_general-qa -> HME_general-qa_merged\n",
      "--- Loading Base Model and Tokenizer ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd58e2e8d214f90aff99049011e83aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lvliuzhenghao/llzh/NC_minor/HME/src/hme/util.py:296: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint_path, \"feature_fuser.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading PEFT Adapter from ../checkpoints/HME_general-qa ---\n",
      "--- Merging Adapter into Base Model ---\n",
      "--- Saving Merged Model to ../checkpoints/HME_general-qa_merged ---\n",
      "No task-specific head to save (task is likely conditional generation).\n",
      "--- Model merged and saved successfully to ../checkpoints/HME_general-qa_merged ---\n"
     ]
    }
   ],
   "source": [
    "# --- Stage 1: Base -> Pretrain Merged ---\n",
    "print(\"--- Stage 1: Merging Pretrain Adapter ---\")\n",
    "path_stage1 = run_merge(BASE_MODEL_PATH, \"HME_comprehension-pretrain\")\n",
    "\n",
    "if path_stage1:\n",
    "    # --- Stage 2: Pretrain Merged -> General QA Model ---\n",
    "    print(\"\\n--- Stage 2: Merging General QA Adapter ---\")\n",
    "    run_merge(path_stage1, \"HME_general-qa\")\n",
    "else:\n",
    "    print(\"\\n Stage 1 failed or skipped. Cannot proceed to Stage 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098c5d1",
   "metadata": {},
   "source": [
    "### 2. Verify Output\n",
    "Let's check the generated merged models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336d4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged models in checkpoints directory:\n",
      "HME_comprehension-pretrain_merged\n",
      "HME_general-qa_merged\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Merged models in checkpoints directory:\")\n",
    "for item in CHECKPOINTS_DIR.iterdir():\n",
    "    if item.is_dir() and item.name.endswith(\"_merged\"):\n",
    "        print(f\"{item.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
